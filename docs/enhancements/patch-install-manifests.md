# Patching Installer Manifests

- [Overview](#overview)
- [Groundwork](#groundwork)
  - [Existing Manifest Patching](#existing-manifest-patching)
  - [`ClusterDeploymentCustomization`](#clusterdeploymentcustomization)
- [API](#api)
  - [`ClusterDeploymentCustomization.Spec.InstallerManifestPatches`](#clusterdeploymentcustomizationspecinstallermanifestpatches)
  - [`ClusterDeployment.Spec.Provisioning.CustomizationRef`](#clusterdeploymentspecprovisioningcustomizationref)
  - [Status](#status)
- [Failure Modes](#failure-modes)

## Overview
Installer accepts spoke configuration via the
[install-config.yaml](https://github.com/openshift/hive/blob/c392ca38fb489267cd7bfbdb3c5a76cc36163686/vendor/github.com/openshift/installer/pkg/types/installconfig.go#L93)
file.
However, some options are only available by editing the OpenShift object manifests generated by
`openshift-install create manifests` prior to feeding them to `openshift-install create cluster`.
Moving forward, installer frequently prefers new features to be enabled via this latter mechanism.

Up to this point, hive has incorporated some piecemeal patching of these manifests at the behest of
specific values passed through via hive APIs.
Example: Extra worker security groups for AWS.
However, this model is awkward, brittle, and not extensible.
What is needed is a mechanism for the hive *user* to specify arbitrary modifications to OpenShift
manifests generated by installer.

## Groundwork

### Existing Manifest Patching
The code that invokes `openshift-install` lives in installmanager.go.
[This block](https://github.com/openshift/hive/blob/9615c77cd786dd82079a9dacd60b64c882f56327/pkg/installmanager/installmanager.go#L882-L921)
contains logic to:
- Walk the `openshift/` directory (one of two created by `openshift-install create manifests`, the
  other being `manifests/`).
- Load yaml files.
- Parse the files as JSON.
- Patch the JSON.
- Convert the JSON back to YAML.
- Write the files back to storage.

### `ClusterDeploymentCustomization`
An earlier feature, [ClusterPool Inventory](clusterpool-inventory.md), invented the concept of
[`ClusterDeploymentCustomization`](https://github.com/openshift/hive/blob/8c79fedae7011e434e8a7ff0fef40994ef92deb2/vendor/github.com/openshift/hive/apis/hive/v1/clusterdeploymentcustomization_types.go),
a hive API used by ClusterPools when specific discrete values (such as reserved IP addresses) must
be injected into the install-config.yaml for each cluster in the pool.
As such, ClusterDeploymentCustomization originally contained only one member,
[`InstallConfigPatches`](https://github.com/openshift/hive/blob/8c79fedae7011e434e8a7ff0fef40994ef92deb2/vendor/github.com/openshift/hive/apis/hive/v1/clusterdeploymentcustomization_types.go#L43),
a list of patches to apply to the install-config.yaml generated by the ClusterPool controller based
on the user-provided template.
Each entry in `InstallConfigPatches` is a
[`PatchEntity`](https://github.com/openshift/hive/blob/8c79fedae7011e434e8a7ff0fef40994ef92deb2/vendor/github.com/openshift/hive/apis/hive/v1/clusterdeploymentcustomization_types.go#L47),
a hive-owned CRD corresponding to an
[RFC 6902 JSON patch](https://datatracker.ietf.org/doc/html/rfc6902).

## API

### `ClusterDeploymentCustomization.Spec.InstallerManifestPatches`

**Example:**
```yaml
apiVersion: v1
kind: ClusterDeploymentCustomization
metadata:
  name: foo-cluster-deployment-customization
  namespace: my-project
spec:
  installerManifestPatches:
    # Add custom labels to all master machine manifests
    - manifestSelector:
        glob: openshift/99_openshift-cluster-api_master-machines-*.yaml
      patches:
        - op: add
          path: metadata/labels/a-custom-label
          value: foo
        - op: add
          path: metadata/labels/b-custom-label
          value: foo
    - ...
  ...
```

- Build on the existing [groundwork](#groundwork), extending `ClusterDeploymentCustomization` to
  expose a new field, `InstallerManifestPatches`.
- Because installer generates multiple manifests, and RFC 6902 syntax can only reference a single
  document at a time, we can't directly use `PatchEntity`; we must use an intervening type to
  identify which file(s) a patch is to be applied to.
  We'll call this `InstallerManifestPatch`.
- Each `InstallerManifestPatch`:
  - Identifies one or more files via subfield `ManifestSelector`.
  - Lists `PatchEntity`s to apply to files thus identified.
    This is the same `PatchEntity` used for `InstallConfigPatches`.
- `ManifestSelector` supports one subfield, `Glob`, which accepts a path matching string as
  supported by golang's [filepath.Glob](https://pkg.go.dev/path/filepath#Glob).
  - We will execute the glob relative to the working directory in which manifests were generated.
  - For security, we will error on any glob starting with `/` or containing `../`.
- `ManifestSelector` is extensible.
  E.g. in the future we may wish to match manifests based on the GVK of the object therein.

### `ClusterDeployment.Spec.Provisioning.CustomizationRef`

**Example:**
```yaml
apiVersion: v1
kind: ClusterDeployment
metadata:
  name: foo-cluster-deployment
  namespace: my-project
spec:
  provisioning:
    installConfigSecretRef: ic-secret
    customizationRef: foo-cluster-deployment-customization
    ...
  ...
```

- Extend `ClusterDeployment`, adding a `CustomizationRef` field at the same level as
  `InstallConfigSecretRef`, i.e. under `Spec.Provisioning`.
- `CustomizationRef` is the name of a `ClusterDeploymentCustomization` in the same namespace as the
  `ClusterDeployment`.
  We will attempt to load it and act on each entry.

### Status
**TBD.**

Where/how do we report the success/failure of
- CDC as a whole? In a new `CD.Status.Conditions[?Type=CustomizationsSuccessful]` or similar?
- Individial `CDC.InstallerManifestPatches`?
  How are we doing it for `InstallConfigPatches` for ClusterPools?

## Failure Modes
**TBD.**

- Is it a failure if `ManifestSelector` matches zero manifests?
- Is it a failure if a `Patches[].Path` matches no paths?
- Failure to execute a patch should probably bubble up as an overall failure.
- Again, take the lead from `InstallConfigPatches` / ClusterPool Inventory.